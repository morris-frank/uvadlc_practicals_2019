
@article{kingma2014,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  shorttitle = {Adam},
  journaltitle = {arXiv preprint arXiv:1412.6980},
  date = {2014},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  file = {/home/morris/docs/eLibrary/Zotero/storage/6Y9EV4V7/Kingma und Ba - 2014 - Adam A method for stochastic optimization.pdf;/home/morris/docs/eLibrary/Zotero/storage/V9TP5BDW/1412.html}
}

@unpublished{hinton2014,
  venue = {{Toronto}},
  title = {Lecture 6 {{Overview}} of Mini-­‐batch Gradient Descent},
  url = {http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
  urldate = {2019-04-29},
  date = {2014},
  author = {Hinton, Geoffrey},
  file = {/home/morris/docs/eLibrary/Zotero/storage/WWIEQQJT/lecture_slides_lec6.pdf}
}

@article{kipf2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.02907},
  primaryClass = {cs, stat},
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  url = {http://arxiv.org/abs/1609.02907},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  urldate = {2019-04-30},
  date = {2016-09-09},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Kipf, Thomas N. and Welling, Max},
  file = {/home/morris/docs/eLibrary/Zotero/storage/5GYMGAE4/Kipf und Welling - 2016 - Semi-Supervised Classification with Graph Convolut.pdf;/home/morris/docs/eLibrary/Zotero/storage/39Z2S7A9/1609.html}
}

@article{zhang2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.03067},
  primaryClass = {cs},
  title = {Multi-{{Granularity Reasoning}} for {{Social Relation Recognition}} from {{Images}}},
  url = {http://arxiv.org/abs/1901.03067},
  abstract = {Discovering social relations in images can make machines better interpret the behavior of human beings. However, automatically recognizing social relations in images is a challenging task due to the significant gap between the domains of visual content and social relation. Existing studies separately process various features such as faces expressions, body appearance, and contextual objects, thus they cannot comprehensively capture the multi-granularity semantics, such as scenes, regional cues of persons, and interactions among persons and objects. To bridge the domain gap, we propose a Multi-Granularity Reasoning framework for social relation recognition from images. The global knowledge and mid-level details are learned from the whole scene and the regions of persons and objects, respectively. Most importantly, we explore the fine-granularity pose keypoints of persons to discover the interactions among persons and objects. Specifically, the pose-guided Person-Object Graph and Person-Pose Graph are proposed to model the actions from persons to object and the interactions between paired persons, respectively. Based on the graphs, social relation reasoning is performed by graph convolutional networks. Finally, the global features and reasoned knowledge are integrated as a comprehensive representation for social relation recognition. Extensive experiments on two public datasets show the effectiveness of the proposed framework.},
  urldate = {2019-04-30},
  date = {2019-01-10},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Zhang, Meng and Liu, Xinchen and Liu, Wu and Zhou, Anfu and Ma, Huadong and Mei, Tao},
  file = {/home/morris/docs/eLibrary/Zotero/storage/Y2KZ7KAM/Zhang et al. - 2019 - Multi-Granularity Reasoning for Social Relation Re.pdf;/home/morris/docs/eLibrary/Zotero/storage/AS95XBAX/1901.html}
}

@article{feng2019,
  title = {Temporal {{Relational Ranking}} for {{Stock Prediction}}},
  volume = {37},
  issn = {1046-8188},
  url = {http://doi.acm.org/10.1145/3309547},
  doi = {10.1145/3309547},
  abstract = {Stock prediction aims to predict the future trends of a stock in order to help investors make good investment decisions. Traditional solutions for stock prediction are based on time-series models. With the recent success of deep neural networks in modeling sequential data, deep learning has become a promising choice for stock prediction. However, most existing deep learning solutions are not optimized toward the target of investment, i.e., selecting the best stock with the highest expected revenue. Specifically, they typically formulate stock prediction as a classification (to predict stock trends) or a regression problem (to predict stock prices). More importantly, they largely treat the stocks as independent of each other. The valuable signal in the rich relations between stocks (or companies), such as two stocks are in the same sector and two companies have a supplier-customer relation, is not considered. In this work, we contribute a new deep learning solution, named Relational Stock Ranking (RSR), for stock prediction. Our RSR method advances existing solutions in two major aspects: (1) tailoring the deep learning models for stock ranking, and (2) capturing the stock relations in a time-sensitive manner. The key novelty of our work is the proposal of a new component in neural network modeling, named Temporal Graph Convolution, which jointly models the temporal evolution and relation network of stocks. To validate our method, we perform back-testing on the historical data of two stock markets, NYSE and NASDAQ. Extensive experiments demonstrate the superiority of our RSR method. It outperforms state-of-the-art stock prediction solutions achieving an average return ratio of 98\% and 71\% on NYSE and NASDAQ, respectively.},
  number = {2},
  journaltitle = {ACM Trans. Inf. Syst.},
  urldate = {2019-04-30},
  date = {2019-03},
  pages = {27:1--27:30},
  keywords = {graph-based learning,learning to rank,Stock prediction},
  author = {Feng, Fuli and He, Xiangnan and Wang, Xiang and Luo, Cheng and Liu, Yiqun and Chua, Tat-Seng},
  file = {/home/morris/docs/eLibrary/Zotero/storage/2CRNZFTL/Feng et al. - 2019 - Temporal Relational Ranking for Stock Prediction.pdf}
}

@article{parisot2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.01738},
  title = {Disease {{Prediction}} Using {{Graph Convolutional Networks}}: {{Application}} to {{Autism Spectrum Disorder}} and {{Alzheimer}}'s {{Disease}}},
  volume = {48},
  issn = {13618415},
  url = {http://arxiv.org/abs/1806.01738},
  doi = {10.1016/j.media.2018.06.001},
  shorttitle = {Disease {{Prediction}} Using {{Graph Convolutional Networks}}},
  abstract = {Graphs are widely used as a natural framework that captures interactions between individual elements represented as nodes in a graph. In medical applications, specifically, nodes can represent individuals within a potentially large population (patients or healthy controls) accompanied by a set of features, while the graph edges incorporate associations between subjects in an intuitive manner. This representation allows to incorporate the wealth of imaging and non-imaging information as well as individual subject features simultaneously in disease classification tasks. Previous graph-based approaches for supervised or unsupervised learning in the context of disease prediction solely focus on pairwise similarities between subjects, disregarding individual characteristics and features, or rather rely on subject-specific imaging feature vectors and fail to model interactions between them. In this paper, we present a thorough evaluation of a generic framework that leverages both imaging and non-imaging information and can be used for brain analysis in large populations. This framework exploits Graph Convolutional Networks (GCNs) and involves representing populations as a sparse graph, where its nodes are associated with imaging-based feature vectors, while phenotypic information is integrated as edge weights. The extensive evaluation explores the effect of each individual component of this framework on disease prediction performance and further compares it to different baselines. The framework performance is tested on two large datasets with diverse underlying data, ABIDE and ADNI, for the prediction of Autism Spectrum Disorder and conversion to Alzheimer's disease, respectively. Our analysis shows that our novel framework can improve over state-of-the-art results on both databases, with 70.4\% classification accuracy for ABIDE and 80.0\% for ADNI.},
  journaltitle = {Medical Image Analysis},
  shortjournal = {Medical Image Analysis},
  urldate = {2019-04-30},
  date = {2018-08},
  pages = {117-130},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Parisot, Sarah and Ktena, Sofia Ira and Ferrante, Enzo and Lee, Matthew and Guerrero, Ricardo and Glocker, Ben and Rueckert, Daniel},
  file = {/home/morris/docs/eLibrary/Zotero/storage/YAE9Y5F7/Parisot et al. - 2018 - Disease Prediction using Graph Convolutional Netwo.pdf;/home/morris/docs/eLibrary/Zotero/storage/B6DFCSEQ/1806.html}
}

@article{he2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.01852},
  primaryClass = {cs},
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human}}-{{Level Performance}} on {{ImageNet Classification}}},
  url = {http://arxiv.org/abs/1502.01852},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
  urldate = {2019-05-01},
  date = {2015-02-06},
  keywords = {Computer Science - Machine Learning,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Artificial Intelligence},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  file = {/home/morris/docs/eLibrary/Zotero/storage/H6RAUYZ9/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf;/home/morris/docs/eLibrary/Zotero/storage/PUHFSBSL/1502.html}
}

@article{zhou2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.08434},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Graph {{Neural Networks}}: {{A Review}} of {{Methods}} and {{Applications}}},
  url = {http://arxiv.org/abs/1812.08434},
  shorttitle = {Graph {{Neural Networks}}},
  abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics system, learning molecular ﬁngerprints, predicting protein interface, and classifying diseases require that a model learns from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures, like the dependency tree of sentences and the scene graph of images, is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are connectionist models that capture the dependence of graphs via message passing between the nodes of graphs. Unlike standard neural networks, graph neural networks retain a state that can represent information from its neighborhood with arbitrary depth. Although the primitive GNNs have been found difﬁcult to train for a ﬁxed point, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful learning with them. In recent years, systems based on graph convolutional network (GCN) and gated graph neural network (GGNN) have demonstrated ground-breaking performance on many tasks mentioned above. In this survey, we provide a detailed review over existing graph neural network models, systematically categorize the applications, and propose four open problems for future research.},
  urldate = {2019-05-04},
  date = {2018-12-20},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Zhou, Jie and Cui, Ganqu and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  file = {/home/morris/docs/eLibrary/Zotero/storage/R623IB6K/Zhou et al. - 2018 - Graph Neural Networks A Review of Methods and App.pdf}
}

@article{doersch2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.05908},
  primaryClass = {cs, stat},
  title = {Tutorial on {{Variational Autoencoders}}},
  url = {http://arxiv.org/abs/1606.05908},
  abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  urldate = {2019-05-10},
  date = {2016-06-19},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Doersch, Carl},
  file = {/home/morris/docs/eLibrary/Zotero/storage/HTGMF9AN/Doersch - 2016 - Tutorial on Variational Autoencoders.pdf;/home/morris/docs/eLibrary/Zotero/storage/NNYCDRR2/1606.html}
}

@article{dinh2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.08803},
  primaryClass = {cs, stat},
  title = {Density Estimation Using {{Real NVP}}},
  url = {http://arxiv.org/abs/1605.08803},
  abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
  urldate = {2019-05-10},
  date = {2016-05-27},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  file = {/home/morris/docs/eLibrary/Zotero/storage/334DEJV3/Dinh et al. - 2016 - Density estimation using Real NVP.pdf;/home/morris/docs/eLibrary/Zotero/storage/UZ3RSZVQ/1605.html}
}

@article{goodfellow2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.2661},
  primaryClass = {cs, stat},
  title = {Generative {{Adversarial Networks}}},
  url = {http://arxiv.org/abs/1406.2661},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  urldate = {2019-05-10},
  date = {2014-06-10},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  file = {/home/morris/docs/eLibrary/Zotero/storage/9GB9Q84A/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf;/home/morris/docs/eLibrary/Zotero/storage/MDGAXVJZ/1406.html}
}

@article{ho2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.00275},
  primaryClass = {cs, stat},
  title = {Flow++: {{Improving Flow}}-{{Based Generative Models}} with {{Variational Dequantization}} and {{Architecture Design}}},
  url = {http://arxiv.org/abs/1902.00275},
  shorttitle = {Flow++},
  abstract = {Flow-based generative models are powerful exact likelihood models with efficient sampling and inference. Despite their computational efficiency, flow-based models generally have much worse density modeling performance compared to state-of-the-art autoregressive models. In this paper, we investigate and improve upon three limiting design choices employed by flow-based models in prior work: the use of uniform noise for dequantization, the use of inexpressive affine flows, and the use of purely convolutional conditioning networks in coupling layers. Based on our findings, we propose Flow++, a new flow-based model that is now the state-of-the-art non-autoregressive model for unconditional density estimation on standard image benchmarks. Our work has begun to close the significant performance gap that has so far existed between autoregressive models and flow-based models. Our implementation is available at https://github.com/aravind0706/flowpp.},
  urldate = {2019-05-10},
  date = {2019-02-01},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Ho, Jonathan and Chen, Xi and Srinivas, Aravind and Duan, Yan and Abbeel, Pieter},
  file = {/home/morris/docs/eLibrary/Zotero/storage/SUCZ76UG/Ho et al. - 2019 - Flow++ Improving Flow-Based Generative Models wit.pdf;/home/morris/docs/eLibrary/Zotero/storage/BVYPB8KD/1902.html}
}

@article{kingma2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.6114},
  primaryClass = {cs, stat},
  title = {Auto-{{Encoding Variational Bayes}}},
  url = {http://arxiv.org/abs/1312.6114},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  urldate = {2019-05-10},
  date = {2013-12-20},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Kingma, Diederik P. and Welling, Max},
  file = {/home/morris/docs/eLibrary/Zotero/storage/BHPA9FLW/Kingma und Welling - 2013 - Auto-Encoding Variational Bayes.pdf;/home/morris/docs/eLibrary/Zotero/storage/MXMNVP3V/1312.html}
}

@article{rezende2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.05770},
  primaryClass = {cs, stat},
  title = {Variational {{Inference}} with {{Normalizing Flows}}},
  url = {http://arxiv.org/abs/1505.05770},
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  urldate = {2019-05-10},
  date = {2015-05-21},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning,Computer Science - Artificial Intelligence,Statistics - Computation,Statistics - Methodology},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  file = {/home/morris/docs/eLibrary/Zotero/storage/VQ8GYZJU/Rezende und Mohamed - 2015 - Variational Inference with Normalizing Flows.pdf;/home/morris/docs/eLibrary/Zotero/storage/IXMTM2K3/1505.html}
}

@incollection{lee2007,
  title = {Efficient Sparse Coding Algorithms},
  url = {http://papers.nips.cc/paper/2979-efficient-sparse-coding-algorithms.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 19},
  publisher = {{MIT Press}},
  urldate = {2019-05-10},
  date = {2007},
  pages = {801--808},
  author = {Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew Y.},
  editor = {Schölkopf, B. and Platt, J. C. and Hoffman, T.},
  file = {/home/morris/docs/eLibrary/Zotero/storage/KRX9BZXI/Lee et al. - 2007 - Efficient sparse coding algorithms.pdf;/home/morris/docs/eLibrary/Zotero/storage/JSDH5EVD/2979-efficient-sparse-coding-algorithms.html}
}

@article{olshausen1996,
  langid = {english},
  title = {Emergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code for Natural Images},
  volume = {381},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/381607a0},
  doi = {10.1038/381607a0},
  abstract = {THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1–4 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7–12. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13–18, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs.},
  number = {6583},
  journaltitle = {Nature},
  urldate = {2019-05-10},
  date = {1996-06},
  pages = {607},
  author = {Olshausen, Bruno A. and Field, David J.},
  file = {/home/morris/docs/eLibrary/Zotero/storage/9FF3QZU5/Olshausen und Field - 1996 - Emergence of simple-cell receptive field propertie.pdf;/home/morris/docs/eLibrary/Zotero/storage/V4622LWY/381607a0.html}
}


