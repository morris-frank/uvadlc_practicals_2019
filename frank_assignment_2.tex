\documentclass{article}
\usepackage[final]{neurips_2019}
\usepackage{morris}

\usepackage{makecell}
\renewcommand\cellalign{tl}

% Some update to the NIPS template
\bibpunct{[}{]}{;}{n}{}{,}
\makeatletter
\renewcommand{\@noticestring}{Deep Learning, Sommer 2019, Universiteit van Amsterdam}
\makeatother

\renewcommand{\thesubsubsection}{\alph{subsubsection})}

\title{Assignment 2. Recurrent Neural Networks and Graph Neural Networks}
\author{%
  Maurice Frank\\
  11650656\\
  \href{mailto:maurice.frank@posteo.de}{maurice.frank@posteo.de} \\
  Code: \href{https://github.com/morris-frank/uvadlc_practicals_2019/tree/master/assignment_2}{github}
}

\begin{document}
\maketitle

\section{Vanilla RNN versus LSTM}
\subsection{RNN derivatives}
Generally we have:
\begin{align}
  \pf{\L}{\B{W}_{ph}}
  &= \pf{\L}{\B{p}} \pf{\B{p}}{\B{W}_{ph}}
\end{align}

With the two partials:
\begin{align}
  \pf{\L}{p_i}
  &= -\Σ_j y_j \pf{\log\hat{y}_j}{p_i}\\
  &= -\Σ_j \f{y_j}{\hat{y}_j}\pf{\hat{y}_j}{p_i}\\
  &= -y_i(1-\hat{y}_i) - \Σ_{j\neq i} \f{y_j}{\hat{y}_j} \· (-\hat{y}_i\hat{y}_j)\\
  &= -y_i + y_i\hat{y}_i + \hat{y}_i\Σ_{j\neq i} y_j\\
  &= \hat{y}_i \left(y_i + \Σ_{j\neq i} y_j\right) - y_i\\
  &= \hat{y}_i - y_i\\
  &\⇔\\
  \pf{\L}{\B{p}}
  &= \B{p} - \B{y}\\
\end{align}
Note here it holds \(\Σ_i y_i = 1\) because of the one-hot encoding.

The second derivative is more direct:
\begin{align}
  \pf{\B{p}}{\B{W}_{ph}}
  &= \B{h}^{(T)}\\
\end{align}

leads finally to:
\begin{align}
  \pf{\L}{\B{W}_{ph}}
  &= (\B{p} - \B{y}) \· \B{h}^{(T)}
\end{align}

The derivative with respect to the hidden weight we write down in its recursive form:
\begin{align}
  \pf{\L}{\B{W}_{hh}}
  &= \pf{\L}{\B{\hat{y}}} \pf{\B{\hat{y}}}{\B{p}} \pf{\B{p}}{\B{h}^{(T)}} \pf{\B{h}^{(T)}}{\B{W}_{hh}}\\
  \pf{\B{h}^{(T)}}{\B{W}_{hh}}
  &= \left(1 - {\B{h}^{(T)}}^2\right)\·\pf{}{\B{W}_{hh}}\B{W}_{hh}\B{h}^{(T-1)}\\
  &= \left(1 - {\B{h}^{(T)}}^2\right)\·\left[\left(\pf{}{\B{W}_{hh}}\B{W}_{hh}\right)\B{h}^{(T-1)} + \B{W}_{hh}\left(\pf{}{\B{W}_{hh}}\B{h}^{(T-1)}\right)\right]\\
  &= \left(1 - {\B{h}^{(T)}}^2\right)\·\left(\B{h}^{(T-1)} + \B{W}_{hh}\pf{\B{h}^{(T-1)}}{\B{W}_{hh}}\right)\\
\end{align}

Because we had to write down \(\pf{\L}{\B{W}_{hh}}\) recursively which we do not do for \(\pf{\L}{\B{W}_{ph}}\) we directly see the different length of the temporal dependencies in the two computational graphs.
For the hidden weights gradient we need to transverse the whole time-sequence while the other one only ever depends on the final hidden state.
This gives problems in practical training probable.
The partial gradients of the hidden weights might be small and they form a product.
The gradient might be vanishing.
In practice this might go so far to reach the numerical limits of floating point arithmetics.

\subsection{Vanilla RNN code}
Find the code inside \texttt{vanilla\_rnn.py} and \texttt{train.py}.

\subsection{Vanilla RNN experiment}
See Figure~\ref{fig:accuracy_loss} for a overview plot of the results and Section~\ref{sub:lstm_practice} for a discussion/comparison of the results.

\subsection{Optimizer}
SGD has problems.
One of them is occurring oscillations in valleys of the loss space.
SGD does not have any \textit{memory} and thus just tries to approximate the currents face's gradient to follow down which might make the path jump around a minimum of the valley.
One change to counter this problem is introducing \textbf{momentum}.
Following the intuition of the physical term, the gradient with momentum gets only changed gradually not sudden in every optimizer step.
This is implemented as a decaying average of gradient updates.
The weights get updated as a weighted sum of the previous update and the new gradient.
A second idea is to tweak the learning rate for each weight and not use a fixed \(\eta\) for all, yielding a \textbf{adaptive learning rate}.
For those weights that change a lot (bounce around some valley) we want to reduce the update step to counteract the bouncing.
This can be seen in the RMSProp~\citep{hinton2014} optimizer as described below:
\begin{align}
  v_t &= \rho v_{t-1} + (1-\rho) \· {(\nabla_{\θ_t}f)}^2\\
  \θ_{t+1} &= \θ_t - \f{\eta}{\sqrt{v_t + \ε}} \· \nabla_{\θ_t}f
\end{align}
\(\rho\) defines the decaying sum. We compute the update but than divide the learning rate \(\eta\) for each weight by the new update.
Thus oscillating weights will get a smaller update.
Adam~\citep{kingma2014} optimizer works quite similar:
\begin{align}
  v_t &= \β_1 \· v_{t-1} - (1 - \β_1) \· \nabla_{\θ_t}f\\
  s_t &= \β_2 \· s_{t-1} - (1 - \β_2) \· {(\nabla_{\θ_t}f)}^2\\
  \θ_{t+1} &= \θ_t - \f{\eta}{\sqrt{s_t + \ε}} \· v_t
\end{align}
We also adapt the learning rate per weight by dividing by the square-root of the  squared gradients.
But here also directly use the momentum but having the decaying sum of weight-wise gradients. \(\β_1 \text{and} \β_2\) are tuneable hyperparameters.

\subsection{LSTM theory}
\subsubsection{LSTM Gates}
\begin{description}
  \item[\I{input modulation gate} \(\B{g}^{(t)}\)] The input modulation gate determines candidate information from the new input (using also the old hidden state).
  We want our state values normalized but need also negative values (otherwise the cell values would only increase) which, as in this case, can be done with a \(\tanh \), squashing the input to \([-1, 1]\).
  \item[\I{input gate} \(\B{i}^{(t)}\)] The input regulates which and how much information of the input of this time step should be included in the cell and hidden state.
  As the input gate regulates the flow it is necessary to have its values bounded to \([0,1]\) which can most directly achieved by squashing the values with the sigmoid.
  \item[\I{forget gate} \(\B{f}^{(t)}\)] The forget gate regulates which and how much information from the old cell state should be disregarded under the new information from the input (and the old hidden state).
  As the forget gate only changes the importance (magnitude) of the information in the cell state it should be in \([0,1]\) which is achieved with the sigmoid.
  \item[\I{output gate} \(\B{o}^{(t)}\)] The output gate regulates which and how much information from the new cell state should go into the new hidden state.
  Again its gating the values from the other tensor which is asking for a range \([0, 1]\) achieved by the sigmoid.
\end{description}

\subsubsection{Number of parameters}
We have given \(\B{x}\∈\ℝ^{T\×d}\) with \(T\) sequence length and \(d\) feature dimension.
Further we have \(n\) hidden units.
Then we have
\begin{equation*}
  4\· (d\·n + n\·n + n)
\end{equation*}
trainable parameters for \textit{one} LSTM cell.
If we want to include the projection onto the classes \(c\) the size increases of course to:
\begin{equation*}
  4\· (d\·n + n\·n + n) + n\·c + c
\end{equation*}

\subsection{LSTM practice}\label{sub:lstm_practice}
Find the code inside \texttt{lstm.py} and \texttt{train.py}.

We train the two models (RNN and LSTM) for palindromes with sizes 5 up to 40.
Both models get a hidden size of 128.
As optimizer we use  RMSprop with a learning rate of \(0.001\) until convergence of the loss.
The weights for both models are initialized with He initialization~\cite{he2015} as it performs well compared to plain normal init.

For a overview of the results check out Figure~\ref{fig:accuracy_loss}.
We see in general that the LSTM is able to learn the palindromes faster and for longer sequences.
The RNN is only able to improve on randomness (\(=\) accuracy of 0.1) up to length 17.
Further the RNN is only able to reach full accuracy for palindromes smaller than 10.
The LSTM reaches perfect accuracy for all lengths but we see that for length of 23 we do not see improvement over randomness until almost 3000.
If run for long enough the LSTM learns full accuracy for all tested lengths but we found that not be the interesting result here.
The LSTM learning slower at higher lengths is explained in that we use the same hyperparameters for all experiments especially the learning rate.
An optimization of the hyperparameters for the LSTM might speed up training for longer sequences.
The experiment clearly shows that the LSTM is more capable of learning longer dependencies.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{assignment_2/part1/palindrome.pdf}
  \caption{\B{Top} the accuracy and \B{bottom} the loss while training.
  Lightness of the color codes the palindrome length ranging from 5 numbers at the lightest color in steps of 2 to 23 numbers at the darkest color.
  (making 9 curves per model). All curves are an average of ten runs and smoothed with a box filter with width 10 for better readability. Note that two of the RNN curves never increase over 0.1 in the accuracy plot.}\label{fig:accuracy_loss}
\end{figure}

\section{Recurrent Nets as Generative Model}
\subsection{Learning English from South Park and the Moses}
In these experiment we train a simple character-level language model with LSTM.
We train from texts from the bible and the television show South Park.

\subsubsection{The Implementation}
Find the code inside \texttt{model.py} and \texttt{train.py}.

We implement the basic character-level language model using two stacked LSTM cells (three for the second experiment).
Both cells are followed by a Dropout unit.
We use a linear projection to project the hidden state from the second LSTM down on the class predictions.
As we get a sequence of characters as input we use an embedding with fixed identity weights (one-hot encoding) to process the input for the LSTM.
We set the hidden size of both cells to 128.
The dropout layer is set at 0.05 dropout probability.

We train the model with the Adam optimizer feeding it sequences of 30 characters from the text in mini-batches of 64.
The initial learning rate is set at 2e-3.
Further we decay the global learning rate every 5000 steps by 0.04.

We preprocess the text files by reducing the set of characters.
All text is made lowercase and line breaks are removed.
We remove special, uncommon characters (e.g. acutes from German and Spanish) replacing them: e.g. ö \(\to\) o and\ ;\ \(\to\)\ ,.

\subsubsection{The results}
For training we feed the model with all transcripts of the television series \textbf{South Park}.
In total the transcript have 813k words.
See Figure~\ref{fig:training_southpark} for the training progress in the first 1e5 steps.

To understand the generative power of our model qualitatively we sample from the language model at different steps during training.
Sampling is done by picking a first character uniformly by random.
Next we iteratively feed the character into the model generate new hidden and cell states.
From the hidden state we can sample a new character under different temperatures.
The new character is saved in the string and fed back to the model with the new hidden and cell state.
We do this until we reached the desired output length.

Following are text samples of length 20, 30, 50 and 120 at step 0, 1e4, 2e4, 3e4, 4e4, 5e4, 1e6, 2e6 and 5e6.
Each step we sample at temperatures \(T\in\) 0, 0.5, 1 and 2.0.
Note that temperatures 0 corresponds to greedy sampling, meaning we pick the character with the highest probability.

\vspace{1em}
\input{assignment_2/part2/results/southpark_2/southpark.txt.generated.tex}
\vspace{1em}

We see some general trends.
At temperature of 2.0 the model never able to form words which suggests that this is too high.
Most interesting we see the model being able to generate almost coherent sentence early on in training as e.g.\ at step 1000 with \(T=0.5\).

In a second experiment we feed the network the \textbf{five books of Moses} from the old testament.
Compared to the previous transcript this text only has 156k words.
We extend the model architecture to check if that increases the generative power.
For that we us \textit{three} stacked LSTM cells.
Further we feed sequences of 40 characters and decay the global learning rate only all 20k steps.
All other parameters are set as before.

Trainings progress was similar to the first experiment so we just give exemplary qualitative results below.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{assignment_2/part2/results/southpark_2/southpark.pdf}
  \caption{Accuracy, Loss and the decaying Learning Rate for the LSTM language model during training using the South Path transcripts.
  Note that accuracy and loss are a bad surrogate for our task as they describe the capability of the model to predict the next character given a string while our actual goal is to get a generative model.}
  \label{fig:training_southpark}
\end{figure}

\section{Graph Neural Networks}
\subsection{Forward Layer}
\subsubsection{Message passing in a GNN}
The \(\hat{A}\) matrix contains the edge information between the nodes in the graph.
This includes the self-connections of all nodes from the identity \(\1_N\).
We update the activations with \(H^{l+1} = \σ(\hat{A}H^{(l)}W^{(l)})\).
So we update the activation associated with one node only for the activations of edges that have non-zero edge weights in \(\hat{A}\).
A node is changed by the nodes its connected to.
In reverse we can see that information in one node can propagate to all connected adjacent nodes in one time step which can be visualized as the message passing over the graph.

\subsubsection{How many layers to propagate a message along three hops?}
In every layer of the GCN you can propagate information along one edge so to let information reach a node three hops away we would need three layers.

\subsection{Applications of GNN}\label{sub:applications_gnn}
In \textit{Multi-Granularity Reasoning for Social Relation Recognition from Images}~\cite{zhang2019} the authors build graphs on images of humans.
The graphs describe first the relationship of a person in the image with its surrounding objects amongst other things other persons in the image.
A second graph represents the pose of each person in the image.
On these two types of graphs they use Graph Convolutional Networks (GCN) to predict the social relationship of these persons.
For example in an image of parent and child the bending pose of the parent and the connection of the two persons in the Person-Object graph lets the method infer their relationship.

Next in \textit{Disease Prediction using Graph Convolutional Networks: Application to Autism Spectrum Disorder and Alzheimer's Disease}~\cite{parisot2018} propose to use GCN for medical image processing.
The nodes of the graph in this setting are features of medical image acquisitions which in their experiments are gathered from structural and functional MRI.
As they predict the health state of multiple individuals at once the graph consists of many image features of multiple persons.
The edges here describe the phenotypical similarity between two individuals which are described by categorical medical data (e.g.\ sex).
The GCN uses this graph to predict the health state of the population.


Lastly in \textit{Temporal Relational Ranking for Stock Prediction}~\cite{feng2019} we seen an application of GCN in stock prediction.
Here the nodes are features capturing the historical information of one stock and the edges capture the relations between two companies stock.
Both historical intra and inter stock features are generated by LSTM from historical stock data.
Again we build a graph from the edges and nodes and use GCN to predict in this case the stocks next day performance at the market.

\subsection{Comparison and Combination of GNN and RNN}
\subsubsection{Comparing GNN and RNN}
Using a RNN assumes our data is orderable along one axis, meaning every node of information is at most connected to one \textit{previous} and one \textit{next} node of information and that no two nodes have the same previous or next node.
For the GNN we assume data whose node of information is related to an irregular number of other nodes.

One major difference for the two is that the GNN can give edges between, relationship between to states, a weight or a feature itself.
For the RNN we assume a equidistant (temporal etc.) edge between each node while for the graph we can adjust this.


\subsubsection{Combining GNN and RNN}
We saw such a combination of RNN and GNN models in Section~\ref{sub:applications_gnn} already. The work on prediction of stock markets used LSTM and GCN in conjunction.
In there each node of graph related to a linear temporal structure, the historic sock data.
As those linear temporal graphs where themselves related to each other one can use a graph model to analyze.

The reverse idea also seems plausible by using a RNN to go over nodes of graphs.
Imaginable might here be an application where the data is organized in a graph, let's say a social graph, but is subject to temporal changes.
A GNN could be used to analyze the state of the graph at one given point while the RNN uses these temporal states and can for example predict future states from that.

\bibliographystyle{unsrtnat}
\bibliography{references}

\end{document}
